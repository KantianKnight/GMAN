{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/r73rtbfx5y57tdt4s_62qsmh0000gn/T/ipykernel_92990/1492739976.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(buffer)\n"
     ]
    }
   ],
   "source": [
    "with open('data/GMAN.pkl', 'rb') as f:\n",
    "\tbuffer = io.BytesIO(f.read())\n",
    "model = torch.load(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darren/Desktop/C4222/GMAN/model/test.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(args.model_file)\n",
      "/Users/darren/Desktop/C4222/GMAN/model/test.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(args.model_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** testing model ****\n",
      "loading model from ./data/GMAN.pkl\n",
      "model restored!\n",
      "evaluating...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainPred, valPred, testPred \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/test.py:42\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(args, log)\u001b[0m\n\u001b[1;32m     40\u001b[0m X \u001b[38;5;241m=\u001b[39m trainX[start_idx: end_idx]\n\u001b[1;32m     41\u001b[0m TE \u001b[38;5;241m=\u001b[39m trainTE[start_idx: end_idx]\n\u001b[0;32m---> 42\u001b[0m pred_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m trainPred\u001b[38;5;241m.\u001b[39mappend(pred_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m X, TE, pred_batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/model_.py:359\u001b[0m, in \u001b[0;36mGMAN.forward\u001b[0;34m(self, X, TE)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# decoder\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTAttBlock_2:\n\u001b[0;32m--> 359\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTE_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# output\u001b[39;00m\n\u001b[1;32m    361\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC_2(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/model_.py:250\u001b[0m, in \u001b[0;36mSTAttBlock.forward\u001b[0;34m(self, X, STE)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, STE):\n\u001b[0;32m--> 250\u001b[0m     HS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatialAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     HT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporalAttention(X, STE)\n\u001b[1;32m    252\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgatedFusion(HS, HT)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/model_.py:130\u001b[0m, in \u001b[0;36mspatialAttention.forward\u001b[0;34m(self, X, STE)\u001b[0m\n\u001b[1;32m    128\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC_q(X)\n\u001b[1;32m    129\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC_k(X)\n\u001b[0;32m--> 130\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFC_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# [K * batch_size, num_step, num_vertex, d]\u001b[39;00m\n\u001b[1;32m    132\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(torch\u001b[38;5;241m.\u001b[39msplit(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/model_.py:56\u001b[0m, in \u001b[0;36mFC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 56\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/C4222/GMAN/model/model_.py:29\u001b[0m, in \u001b[0;36mconv2d_.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, ([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_size[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[0;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Ensure data is also moved to the same device\n",
    "trainX, trainTE, trainY = trainX.to(device), trainTE.to(device), trainY.to(device)\n",
    "valX, valTE, valY = valX.to(device), valTE.to(device), valY.to(device)\n",
    "testX, testTE, testY = testX.to(device), testTE.to(device), testY.to(device)\n",
    "\n",
    "trainPred, valPred, testPred = test(args, log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original arrangement: [51, 14, 60, 20, 23, 2, 21, 52, 1, 29, 37, 1, 63, 59, 20, 32, 57, 21, 48, 58, 41, 59, 14, 61, 61, 46, 61, 50, 54, 63, 2, 50, 6, 20, 38, 17, 3, 59, 13, 8, 52, 1, 59, 43, 7, 46, 34, 35, 49, 3, 1, 5, 53, 3, 53, 62, 17, 43, 33, 61, 13, 47, 14, 61, 39, 52, 23, 25, 59, 40, 28, 14, 44, 64, 8, 0, 7, 62, 10, 7, 34, 34, 32, 4, 40, 27, 6, 11, 33, 32, 47, 22, 61, 36, 43, 34, 64, 46, 2, 0, 4, 13, 26, 8, 14, 41, 50, 62, 51, 3, 22, 14, 42, 28, 35, 12, 31, 58, 27, 41, 44, 61, 56, 5, 27, 27, 43, 29, 61, 61, 0, 26, 61, 2, 26, 8, 61, 36, 50, 43, 23, 58, 31, 51, 61, 57, 51, 11, 38, 1, 2, 55, 58, 1, 1, 53, 0, 18, 1, 52, 43, 31, 31, 54, 55, 16, 37, 23, 10, 15, 58, 2, 19, 58, 35, 18, 18, 19, 51, 32, 39, 38, 0, 10, 56, 49, 22, 30, 41, 6, 15, 59, 1, 0, 47, 11, 36, 31, 8, 18, 47, 2, 19, 23, 53, 32, 23, 35, 37, 24, 17, 53, 34, 60, 40, 32, 32, 13, 20, 47, 19, 7, 6, 16, 32, 47, 58, 21, 29, 37, 50, 53, 7, 26, 26, 20, 29, 27, 63, 60, 47, 18, 3, 34, 63, 48, 16, 43, 29, 45, 5, 36, 23, 45, 52, 59, 62, 31, 32, 17, 24, 53, 57, 45, 23, 31, 46, 22, 26, 1, 16, 32, 8, 42, 47, 38, 41, 25, 49, 24, 23, 12, 59, 6, 56, 35, 44, 19, 64, 7, 15, 13, 14, 31, 62, 50, 24, 57, 62, 61, 21, 57, 57, 48, 51, 41, 14, 53, 59, 7, 52, 59, 4, 5, 46, 54, 39, 51, 15, 12, 29, 18, 16, 62, 18]\n",
      "Mapped result: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 9, 12, 13, 4, 14, 15, 7, 16, 17, 18, 13, 2, 19, 19, 20, 19, 21, 22, 12, 6, 21, 23, 4, 24, 25, 26, 13, 27, 28, 8, 9, 13, 29, 30, 20, 31, 32, 33, 26, 9, 34, 35, 26, 35, 36, 25, 29, 37, 19, 27, 38, 2, 19, 39, 8, 5, 40, 13, 41, 42, 2, 43, 44, 28, 45, 30, 36, 46, 30, 31, 31, 14, 47, 41, 48, 23, 49, 37, 14, 38, 50, 19, 51, 29, 31, 44, 20, 6, 45, 47, 27, 52, 28, 2, 18, 21, 36, 1, 26, 50, 2, 53, 42, 32, 54, 55, 17, 48, 18, 43, 19, 56, 34, 48, 48, 29, 10, 19, 19, 45, 52, 19, 6, 52, 28, 19, 51, 21, 29, 5, 17, 55, 1, 19, 15, 1, 49, 24, 9, 6, 57, 17, 9, 9, 35, 45, 58, 9, 8, 29, 55, 55, 22, 57, 59, 11, 5, 46, 60, 17, 6, 61, 17, 32, 58, 58, 61, 1, 14, 39, 24, 45, 46, 56, 33, 50, 62, 18, 23, 60, 13, 9, 45, 38, 49, 51, 55, 28, 58, 38, 6, 61, 5, 35, 14, 5, 32, 11, 63, 25, 35, 31, 3, 41, 14, 14, 27, 4, 38, 61, 30, 23, 59, 14, 38, 17, 7, 10, 11, 21, 35, 30, 52, 52, 4, 10, 48, 12, 3, 38, 58, 26, 31, 12, 16, 59, 29, 10, 64, 34, 51, 5, 64, 8, 13, 36, 55, 14, 25, 63, 35, 15, 64, 5, 55, 20, 50, 52, 9, 59, 14, 28, 53, 38, 24, 18, 40, 33, 63, 5, 54, 13, 23, 56, 32, 43, 61, 44, 30, 60, 27, 2, 55, 36, 21, 63, 15, 36, 19, 7, 15, 15, 16, 1, 18, 2, 35, 13, 30, 8, 13, 47, 34, 20, 22, 39, 1, 60, 54, 10, 58, 59, 36, 58]\n"
     ]
    }
   ],
   "source": [
    "def map_arrangement(arr):\n",
    "    # Initialize the result array with None\n",
    "    result = [None] * len(arr)\n",
    "\n",
    "    # Variable to track the next number to assign\n",
    "    num = 1  # Starting from 1\n",
    "\n",
    "    # First pass to assign unique numbers\n",
    "    for i, number in enumerate(arr):\n",
    "        if result[i] is None:\n",
    "            result[i] = num  # Assign the current number\n",
    "            for j in range(i + 1, len(arr)):  # Start from the next index\n",
    "                if arr[j] == number and result[j] is None:\n",
    "                    result[j] = num  # Assign the same number to duplicates\n",
    "            num += 1  # Increment for the next unique number\n",
    "\n",
    "    return result\n",
    "                    \n",
    "\n",
    "# Example usage\n",
    "import numpy as np\n",
    "\n",
    "# Create a list of 325 random integers from 0 to 64\n",
    "np.random.seed(42)  # For reproducibility\n",
    "arrangement = np.random.randint(0, 65, size=325).tolist()\n",
    "\n",
    "mapped_result = map_arrangement(arrangement)\n",
    "\n",
    "print(\"Original arrangement:\", arrangement)\n",
    "print(\"Mapped result:\", mapped_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkmeansc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from model.kmeansc import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import io\n",
    "import argparse\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils_ import log_string, plot_train_val_loss\n",
    "from utils.utils_ import count_parameters, load_data\n",
    "\n",
    "\n",
    "from model.model_ import GMAN\n",
    "from model.train import train\n",
    "from model.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser()\n\u001b[0;32m      2\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--time_slot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma time step is 5 mins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--num_his\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory steps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--time_slot', type=int, default=5,\n",
    "                    help='a time step is 5 mins')\n",
    "parser.add_argument('--num_his', type=int, default=12,\n",
    "                    help='history steps')\n",
    "parser.add_argument('--num_pred', type=int, default=12,\n",
    "                    help='prediction steps')\n",
    "parser.add_argument('--L', type=int, default=1,\n",
    "                    help='number of STAtt Blocks')\n",
    "parser.add_argument('--K', type=int, default=8,\n",
    "                    help='number of attention heads')\n",
    "parser.add_argument('--d', type=int, default=8,\n",
    "                    help='dims of each head attention outputs')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7,\n",
    "                    help='training set [default : 0.7]')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.1,\n",
    "                    help='validation set [default : 0.1]')\n",
    "parser.add_argument('--test_ratio', type=float, default=0.2,\n",
    "                    help='testing set [default : 0.2]')\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--max_epoch', type=int, default=1,\n",
    "                    help='epoch to run')\n",
    "parser.add_argument('--patience', type=int, default=10,\n",
    "                    help='patience for early stop')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=10,\n",
    "                    help='decay epoch')\n",
    "parser.add_argument('--traffic_file', default='./data/pems-bay.h5',\n",
    "                    help='traffic file')\n",
    "parser.add_argument('--SE_file', default='./data/SE(PeMS).txt',\n",
    "                    help='spatial embedding file')\n",
    "parser.add_argument('--model_file', default='./data/GMAN.pkl',\n",
    "                    help='save the model to disk')\n",
    "parser.add_argument('--log_file', default='./data/log',\n",
    "                    help='log file')\n",
    "args, unknown = parser.parse_known_args()\n",
    "log = open(args.log_file, 'w')\n",
    "log_string(log, str(args)[10: -1])\n",
    "T = 24 * 60 // args.time_slot  # Number of time steps in one day\n",
    "# load data\n",
    "log_string(log, 'loading data...')\n",
    "(trainX, trainTE, trainY, valX, valTE, valY, testX, testTE,\n",
    "testY, SE, mean, std) = load_data(args)\n",
    "log_string(log, f'trainX: {trainX.shape}\\t\\t trainY: {trainY.shape}')\n",
    "log_string(log, f'valX:   {valX.shape}\\t\\tvalY:   {valY.shape}')\n",
    "log_string(log, f'testX:   {testX.shape}\\t\\ttestY:   {testY.shape}')\n",
    "log_string(log, f'mean:   {mean:.4f}\\t\\tstd:   {std:.4f}')\n",
    "log_string(log, 'data loaded!')\n",
    "del trainX, trainTE, valX, valTE, testX, testTE, mean, std\n",
    "# build model\n",
    "log_string(log, 'compiling model...')\n",
    "\n",
    "model = GMAN(SE, args, bn_decay=0.1)\n",
    "loss_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=args.decay_epoch,\n",
    "                                      gamma=0.9)\n",
    "parameters = count_parameters(model)\n",
    "log_string(log, 'trainable parameters: {:,}'.format(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = equal_size_kmeans(SE, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT = torch.arange(560).reshape(2, 10, 4, 7)\n",
    "# INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## THIS IS TO CREATE THE MATRIX PERMUTATION \n",
    "\n",
    "# tensor = torch.zeros((325, 12, 8))\n",
    "# for i in range(tensor.size(0)):\n",
    "#     tensor[i] = i  # Fill the i-th slice with the index value\n",
    "\n",
    "# # Print the tensor\n",
    "# print(\"Tensor:\")\n",
    "# print(tensor)\n",
    "# print(tensor.shape)\n",
    "\n",
    "# # Given grouping tensor\n",
    "# # Define the number of repetitions and the range of values\n",
    "# repetitions = 5\n",
    "# max_value = 65\n",
    "\n",
    "# # Create a tensor with the desired values\n",
    "# grouping = torch.arange(1, max_value + 1).repeat(repetitions)\n",
    "# print(grouping.shape)\n",
    "\n",
    "# # Initialize a dictionary to keep track of counts for each number\n",
    "# count = {}\n",
    "\n",
    "# # Initialize a new tensor to store the transformed values\n",
    "# transformed = torch.empty_like(grouping)\n",
    "\n",
    "# for index, value in enumerate(grouping):\n",
    "#     # If the value is not in the count dictionary, initialize it\n",
    "#     if value.item() not in count:\n",
    "#         count[value.item()] = 0\n",
    "    \n",
    "#     # Increment the count for this value\n",
    "#     count[value.item()] += 1\n",
    "    \n",
    "#     # Calculate the new value using the formula\n",
    "#     k = count[value.item()]  # k is the current count of the number\n",
    "#     new_value = 5*(value.item()-1) + k - 1\n",
    "    \n",
    "#     # Assign the new value to the transformed tensor\n",
    "#     transformed[index] = new_value\n",
    "\n",
    "# # Print the original and transformed tensors\n",
    "# print(\"Original Tensor:\", grouping)\n",
    "# print(\"Transformed Tensor:\", transformed)\n",
    "\n",
    "# final_matrix = tensor[[]]\n",
    "# print(final_matrix)\n",
    "\n",
    "# # Create a matrix of zeros with shape (10, 10)\n",
    "# matrix_size = 325\n",
    "# matrix = torch.zeros((matrix_size, matrix_size), dtype=torch.int)\n",
    "\n",
    "# # Iterate through the tensor and set the corresponding positions to 1\n",
    "# for row, value in enumerate(transformed):\n",
    "#     matrix[row, value] = 1  # Set the (row, value + 1) position to 1\n",
    "\n",
    "# # Print the tensor\n",
    "# print(\"Tensor:\")\n",
    "# print(tensor)\n",
    "\n",
    "# # Print the resulting matrix\n",
    "# print(\"Resulting Matrix:\")\n",
    "# print(matrix)\n",
    "\n",
    "# C = torch.matmul(tensor, matrix.float().t())\n",
    "\n",
    "# print(C)\n",
    "\n",
    "# orig = torch.matmul(C, matrix.float())\n",
    "\n",
    "# print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         ...,\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.]],\n",
      "\n",
      "        [[  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         ...,\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         ...,\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.]],\n",
      "\n",
      "        [[323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         ...,\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.]],\n",
      "\n",
      "        [[324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         ...,\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.]]])\n",
      "torch.Size([325, 12, 8])\n",
      "torch.Size([325])\n",
      "Original Tensor: tensor([43, 48,  5,  5, 42,  1,  1, 42, 10, 10, 10, 60, 45, 45,  0, 32, 60, 44,\n",
      "        21, 61, 24, 61, 24, 12, 12, 12, 12, 12, 21, 34, 44, 51, 16, 34, 44, 44,\n",
      "        47, 34, 55, 41, 45, 51, 55, 55, 51, 36, 50, 23, 64,  9, 64, 57, 19, 30,\n",
      "        57, 19, 23, 50, 19, 60, 23, 50, 46, 40, 14, 63, 47, 56, 50, 14, 19, 55,\n",
      "        64, 60, 16, 49, 58, 41, 46,  2, 14, 34, 54, 50, 18, 47, 11, 60, 40, 40,\n",
      "        30, 47, 15, 51, 54, 11, 11, 63, 40,  0,  0, 47, 49, 39, 49, 37, 49, 18,\n",
      "        59, 15, 37,  2, 14,  0, 57, 61, 41, 40, 39, 59, 59, 57,  0, 54, 59, 17,\n",
      "        23, 37, 18, 56, 46, 34, 19,  4, 15,  4, 51, 39, 23, 56, 32, 63, 61,  9,\n",
      "         5, 36, 54, 39, 58,  7, 17, 27, 11, 38, 14, 22, 49,  4, 45, 15, 55, 11,\n",
      "        29,  8, 46, 39, 17,  7, 38, 32, 56,  9, 37, 53, 54, 64, 30, 32, 16, 32,\n",
      "        25,  2, 37, 43, 63, 46, 25, 18,  1, 16, 56, 61, 36, 64, 25, 33, 25, 25,\n",
      "         1, 15, 27,  7, 30, 48,  7, 17, 20, 22, 43,  6, 45,  4, 20,  9, 31, 28,\n",
      "        28, 59, 43, 42, 20, 22,  7, 18, 26, 22, 30, 58, 42, 20, 22, 10, 28, 29,\n",
      "        13, 20, 29,  9, 44, 28, 63, 13, 17, 13,  4, 29, 31, 31, 28, 29, 16, 24,\n",
      "        31, 26, 24, 53,  3, 53,  3, 62, 13, 43,  6, 57, 36, 62,  2,  1, 62, 13,\n",
      "        36,  8,  8, 26, 53, 53,  3, 21, 38, 24, 62, 62, 58, 35, 52,  3, 31, 52,\n",
      "         3, 33, 21, 35, 33, 35, 38, 41, 52, 26,  6,  6, 10, 21, 41, 38, 35, 26,\n",
      "        48, 27, 27, 48, 35,  6,  5, 27, 52,  8, 33, 48,  8, 52, 33,  2, 42,  5,\n",
      "        58])\n",
      "Back to Normal List: [210, 235, 20, 21, 205, 0, 1, 206, 45, 46, 47, 295, 220, 221, -5, 155, 296, 215, 100, 300, 115, 301, 116, 55, 56, 57, 58, 59, 101, 165, 216, 250, 75, 166, 217, 218, 230, 167, 270, 200, 222, 251, 271, 272, 252, 175, 245, 110, 315, 40, 316, 280, 90, 145, 281, 91, 111, 246, 92, 297, 112, 247, 225, 195, 65, 310, 231, 275, 248, 66, 93, 273, 317, 298, 76, 240, 285, 201, 226, 5, 67, 168, 265, 249, 85, 232, 50, 299, 196, 197, 146, 233, 70, 253, 266, 51, 52, 311, 198, -4, -3, 234, 241, 190, 242, 180, 243, 86, 290, 71, 181, 6, 68, -2, 282, 302, 202, 199, 191, 291, 292, 283, -1, 267, 293, 80, 113, 182, 87, 276, 227, 169, 94, 15, 72, 16, 254, 192, 114, 277, 156, 312, 303, 41, 22, 176, 268, 193, 286, 30, 81, 130, 53, 185, 69, 105, 244, 17, 223, 73, 274, 54, 140, 35, 228, 194, 82, 31, 186, 157, 278, 42, 183, 260, 269, 318, 147, 158, 77, 159, 120, 7, 184, 211, 313, 229, 121, 88, 2, 78, 279, 304, 177, 319, 122, 160, 123, 124, 3, 74, 131, 32, 148, 236, 33, 83, 95, 106, 212, 25, 224, 18, 96, 43, 150, 135, 136, 294, 213, 207, 97, 107, 34, 89, 125, 108, 149, 287, 208, 98, 109, 48, 137, 141, 60, 99, 142, 44, 219, 138, 314, 61, 84, 62, 19, 143, 151, 152, 139, 144, 79, 117, 153, 126, 118, 261, 10, 262, 11, 305, 63, 214, 26, 284, 178, 306, 8, 4, 307, 64, 179, 36, 37, 127, 263, 264, 12, 102, 187, 119, 308, 309, 288, 170, 255, 13, 154, 256, 14, 161, 103, 171, 162, 172, 188, 203, 257, 128, 27, 28, 49, 104, 204, 189, 173, 129, 237, 132, 133, 238, 174, 29, 23, 134, 258, 38, 163, 239, 39, 259, 164, 9, 209, 24, 289]\n",
      "Transformed List: [5, 6, 188, 198, 267, 79, 111, 181, 266, 321, 256, 258, 276, 285, 288, 133, 135, 157, 211, 244, 2, 3, 144, 312, 323, 209, 262, 298, 299, 311, 149, 167, 201, 204, 222, 163, 271, 272, 315, 318, 49, 143, 171, 213, 237, 8, 9, 10, 231, 300, 86, 95, 96, 152, 161, 23, 24, 25, 26, 27, 234, 241, 243, 260, 269, 64, 69, 80, 112, 154, 92, 109, 134, 159, 199, 32, 74, 178, 189, 250, 125, 150, 166, 205, 242, 84, 107, 128, 187, 223, 52, 55, 58, 70, 132, 206, 212, 220, 229, 235, 18, 28, 277, 290, 301, 155, 207, 221, 225, 230, 47, 56, 60, 126, 138, 20, 22, 251, 254, 279, 180, 186, 194, 196, 197, 224, 253, 273, 297, 305, 151, 200, 307, 308, 313, 215, 216, 232, 239, 248, 162, 233, 236, 245, 249, 53, 90, 176, 202, 226, 214, 246, 247, 252, 286, 15, 140, 169, 177, 179, 195, 289, 292, 316, 320, 29, 33, 37, 81, 131, 283, 291, 293, 304, 310, 45, 145, 192, 264, 270, 105, 110, 127, 172, 182, 153, 168, 278, 294, 303, 103, 118, 137, 147, 165, 63, 88, 89, 98, 117, 39, 77, 116, 295, 302, 4, 7, 219, 228, 322, 0, 183, 208, 218, 261, 17, 30, 34, 35, 238, 12, 13, 40, 158, 210, 62, 78, 130, 164, 185, 36, 66, 85, 91, 101, 1, 203, 306, 309, 317, 75, 102, 104, 106, 156, 46, 57, 61, 68, 83, 31, 41, 44, 93, 136, 284, 287, 296, 314, 319, 173, 255, 257, 274, 275, 82, 94, 123, 146, 174, 38, 42, 43, 71, 160, 67, 129, 139, 170, 190, 51, 54, 114, 121, 263, 76, 148, 227, 282, 324, 108, 119, 120, 124, 217, 11, 16, 59, 73, 87, 19, 21, 115, 142, 191, 259, 265, 268, 280, 281, 65, 97, 141, 184, 240, 48, 50, 72, 175, 193]\n",
      "torch.Size([325])\n",
      "325\n",
      "320\n",
      "tensor([[[  5.,   5.,   5.,  ...,   5.,   5.,   5.],\n",
      "         [  5.,   5.,   5.,  ...,   5.,   5.,   5.],\n",
      "         [  5.,   5.,   5.,  ...,   5.,   5.,   5.],\n",
      "         ...,\n",
      "         [  5.,   5.,   5.,  ...,   5.,   5.,   5.],\n",
      "         [  5.,   5.,   5.,  ...,   5.,   5.,   5.],\n",
      "         [  5.,   5.,   5.,  ...,   5.,   5.,   5.]],\n",
      "\n",
      "        [[  6.,   6.,   6.,  ...,   6.,   6.,   6.],\n",
      "         [  6.,   6.,   6.,  ...,   6.,   6.,   6.],\n",
      "         [  6.,   6.,   6.,  ...,   6.,   6.,   6.],\n",
      "         ...,\n",
      "         [  6.,   6.,   6.,  ...,   6.,   6.,   6.],\n",
      "         [  6.,   6.,   6.,  ...,   6.,   6.,   6.],\n",
      "         [  6.,   6.,   6.,  ...,   6.,   6.,   6.]],\n",
      "\n",
      "        [[188., 188., 188.,  ..., 188., 188., 188.],\n",
      "         [188., 188., 188.,  ..., 188., 188., 188.],\n",
      "         [188., 188., 188.,  ..., 188., 188., 188.],\n",
      "         ...,\n",
      "         [188., 188., 188.,  ..., 188., 188., 188.],\n",
      "         [188., 188., 188.,  ..., 188., 188., 188.],\n",
      "         [188., 188., 188.,  ..., 188., 188., 188.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 72.,  72.,  72.,  ...,  72.,  72.,  72.],\n",
      "         [ 72.,  72.,  72.,  ...,  72.,  72.,  72.],\n",
      "         [ 72.,  72.,  72.,  ...,  72.,  72.,  72.],\n",
      "         ...,\n",
      "         [ 72.,  72.,  72.,  ...,  72.,  72.,  72.],\n",
      "         [ 72.,  72.,  72.,  ...,  72.,  72.,  72.],\n",
      "         [ 72.,  72.,  72.,  ...,  72.,  72.,  72.]],\n",
      "\n",
      "        [[175., 175., 175.,  ..., 175., 175., 175.],\n",
      "         [175., 175., 175.,  ..., 175., 175., 175.],\n",
      "         [175., 175., 175.,  ..., 175., 175., 175.],\n",
      "         ...,\n",
      "         [175., 175., 175.,  ..., 175., 175., 175.],\n",
      "         [175., 175., 175.,  ..., 175., 175., 175.],\n",
      "         [175., 175., 175.,  ..., 175., 175., 175.]],\n",
      "\n",
      "        [[193., 193., 193.,  ..., 193., 193., 193.],\n",
      "         [193., 193., 193.,  ..., 193., 193., 193.],\n",
      "         [193., 193., 193.,  ..., 193., 193., 193.],\n",
      "         ...,\n",
      "         [193., 193., 193.,  ..., 193., 193., 193.],\n",
      "         [193., 193., 193.,  ..., 193., 193., 193.],\n",
      "         [193., 193., 193.,  ..., 193., 193., 193.]]])\n",
      "tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         ...,\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.]],\n",
      "\n",
      "        [[  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         ...,\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "         [  2.,   2.,   2.,  ...,   2.,   2.,   2.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         ...,\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.],\n",
      "         [322., 322., 322.,  ..., 322., 322., 322.]],\n",
      "\n",
      "        [[323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         ...,\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.],\n",
      "         [323., 323., 323.,  ..., 323., 323., 323.]],\n",
      "\n",
      "        [[324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         ...,\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.],\n",
      "         [324., 324., 324.,  ..., 324., 324., 324.]]])\n"
     ]
    }
   ],
   "source": [
    "## THIS IS TO CREATE THE MATRIX PERMUTATION \n",
    "\n",
    "tensor = torch.zeros((325, 12, 8))\n",
    "for i in range(tensor.size(0)):\n",
    "    tensor[i] = i  # Fill the i-th slice with the index value\n",
    "\n",
    "# Print the tensor\n",
    "print(\"Tensor:\")\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "\n",
    "# Given grouping tensor\n",
    "# Define the number of repetitions and the range of values\n",
    "repetitions = 5\n",
    "max_value = 65\n",
    "\n",
    "# Create a tensor with the desired values\n",
    "grouping = torch.tensor([\n",
    "    43, 48, 5, 5, 42, 1, 1, 42, 10, 10,\n",
    "    10, 60, 45, 45, 0, 32, 60, 44, 21, 61,\n",
    "    24, 61, 24, 12, 12, 12, 12, 12, 21, 34,\n",
    "    44, 51, 16, 34, 44, 44, 47, 34, 55, 41,\n",
    "    45, 51, 55, 55, 51, 36, 50, 23, 64, 9,\n",
    "    64, 57, 19, 30, 57, 19, 23, 50, 19, 60,\n",
    "    23, 50, 46, 40, 14, 63, 47, 56, 50, 14,\n",
    "    19, 55, 64, 60, 16, 49, 58, 41, 46, 2,\n",
    "    14, 34, 54, 50, 18, 47, 11, 60, 40, 40,\n",
    "    30, 47, 15, 51, 54, 11, 11, 63, 40, 0,\n",
    "    0, 47, 49, 39, 49, 37, 49, 18, 59, 15,\n",
    "    37, 2, 14, 0, 57, 61, 41, 40, 39, 59,\n",
    "    59, 57, 0, 54, 59, 17, 23, 37, 18, 56,\n",
    "    46, 34, 19, 4, 15, 4, 51, 39, 23, 56,\n",
    "    32, 63, 61, 9, 5, 36, 54, 39, 58, 7,\n",
    "    17, 27, 11, 38, 14, 22, 49, 4, 45, 15,\n",
    "    55, 11, 29, 8, 46, 39, 17, 7, 38, 32,\n",
    "    56, 9, 37, 53, 54, 64, 30, 32, 16, 32,\n",
    "    25, 2, 37, 43, 63, 46, 25, 18, 1, 16,\n",
    "    56, 61, 36, 64, 25, 33, 25, 25, 1, 15,\n",
    "    27, 7, 30, 48, 7, 17, 20, 22, 43, 6,\n",
    "    45, 4, 20, 9, 31, 28, 28, 59, 43, 42,\n",
    "    20, 22, 7, 18, 26, 22, 30, 58, 42, 20,\n",
    "    22, 10, 28, 29, 13, 20, 29, 9, 44, 28,\n",
    "    63, 13, 17, 13, 4, 29, 31, 31, 28, 29,\n",
    "    16, 24, 31, 26, 24, 53, 3, 53, 3, 62,\n",
    "    13, 43, 6, 57, 36, 62, 2, 1, 62, 13,\n",
    "    36, 8, 8, 26, 53, 53, 3, 21, 38, 24,\n",
    "    62, 62, 58, 35, 52, 3, 31, 52, 3, 33,\n",
    "    21, 35, 33, 35, 38, 41, 52, 26, 6, 6,\n",
    "    10, 21, 41, 38, 35, 26, 48, 27, 27, 48,\n",
    "    35, 6, 5, 27, 52, 8, 33, 48, 8, 52,\n",
    "    33, 2, 42, 5, 58\n",
    "])\n",
    "print(grouping.shape)\n",
    "\n",
    "# Initialize a dictionary to keep track of counts for each number\n",
    "count = {}\n",
    "\n",
    "# Initialize a new list to store the transformed values\n",
    "back_to_normal = []  # Create an empty list\n",
    "\n",
    "for index, value in enumerate(grouping):\n",
    "    # If the value is not in the count dictionary, initialize it\n",
    "    if value.item() not in count:\n",
    "        count[value.item()] = 0\n",
    "    \n",
    "    # Increment the count for this value\n",
    "    count[value.item()] += 1\n",
    "    \n",
    "    # Calculate the new value using the formula\n",
    "    k = count[value.item()]  # k is the current count of the number\n",
    "    new_value = 5 * (value.item() - 1) + k - 1\n",
    "    \n",
    "    # Append the new value to the list\n",
    "    back_to_normal.append(new_value)\n",
    "    \n",
    "# Initialize an empty list to hold the indices\n",
    "transform = []\n",
    "\n",
    "# Iterate through each number from 1 to 65\n",
    "for number in range(1, 66):\n",
    "    # Get the indices where tensor is equal to the current number\n",
    "    indices = (grouping == number).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Extend the transform list with the indices\n",
    "    transform.extend(indices.tolist())\n",
    "\n",
    "\n",
    "# Print the original and transformed tensors\n",
    "print(\"Original Tensor:\", grouping)\n",
    "print(\"Back to Normal List:\", back_to_normal)\n",
    "print(\"Transformed List:\", transform)\n",
    "\n",
    "print(grouping.shape)\n",
    "print(len(back_to_normal))\n",
    "print(len(transform))\n",
    "\n",
    "\n",
    "final_matrix = tensor[transform]\n",
    "print(final_matrix)\n",
    "\n",
    "normal_mat = final_matrix[back_to_normal]\n",
    "print(normal_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]]])\n",
      "Original Tensor:\n",
      " tensor([[[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]],\n",
      "\n",
      "        [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "         [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "         [ 3,  3,  3,  ...,  3,  3,  3],\n",
      "         ...,\n",
      "         [58, 58, 58,  ..., 58, 58, 58],\n",
      "         [59, 59, 59,  ..., 59, 59, 59],\n",
      "         [60, 60, 60,  ..., 60, 60, 60]]])\n",
      "New Tensor:\n",
      " tensor([[[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  ...,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  ...,  3.,  3.,  3.],\n",
      "         ...,\n",
      "         [40., 40., 40.,  ..., 40., 40., 40.],\n",
      "         [41., 41., 41.,  ..., 41., 41., 41.],\n",
      "         [42., 42., 42.,  ..., 42., 42., 42.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create an original tensor of shape (325, 60, 8) filled with values from 1 to 60\n",
    "original_tensor = torch.arange(1, 61).unsqueeze(1).expand(-1, 8).unsqueeze(0).expand(325, -1, -1)\n",
    "\n",
    "# Print the original tensor\n",
    "print(\"Original Tensor:\\n\", original_tensor)\n",
    "\n",
    "# Create a new tensor of shape (325, 24, 8)\n",
    "new_tensor = torch.zeros(325, 24, 8)\n",
    "\n",
    "# Keep the first 12 slices the same\n",
    "new_tensor[:, :, :12, :] = original_tensor[:, :, :12, :]\n",
    "\n",
    "# Calculate the sums for the 13th to 24th slices\n",
    "for i in range(12, 24):\n",
    "    for j in range(4):\n",
    "        new_tensor[:, :, i, :] += original_tensor[:, :, i + 12 * j, :]\n",
    "    new_tensor[:, :, i, :] /= 4\n",
    "        \n",
    "# The new_tensor is now populated with the required sums\n",
    "print(\"Original Tensor:\\n\", original_tensor)\n",
    "print(\"New Tensor:\\n\", new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43, 48,  5,  5, 42,  1,  1, 42, 10, 10, 10, 60, 45, 45,  0, 32, 60, 44,\n",
      "        21, 61, 24, 61, 24, 12, 12, 12, 12, 12, 21, 34, 44, 51, 16, 34, 44, 44,\n",
      "        47, 34, 55, 41, 45, 51, 55, 55, 51, 36, 50, 23, 64,  9, 64, 57, 19, 30,\n",
      "        57, 19, 23, 50, 19, 60, 23, 50, 46, 40, 14, 63, 47, 56, 50, 14, 19, 55,\n",
      "        64, 60, 16, 49, 58, 41, 46,  2, 14, 34, 54, 50, 18, 47, 11, 60, 40, 40,\n",
      "        30, 47, 15, 51, 54, 11, 11, 63, 40,  0,  0, 47, 49, 39, 49, 37, 49, 18,\n",
      "        59, 15, 37,  2, 14,  0, 57, 61, 41, 40, 39, 59, 59, 57,  0, 54, 59, 17,\n",
      "        23, 37, 18, 56, 46, 34, 19,  4, 15,  4, 51, 39, 23, 56, 32, 63, 61,  9,\n",
      "         5, 36, 54, 39, 58,  7, 17, 27, 11, 38, 14, 22, 49,  4, 45, 15, 55, 11,\n",
      "        29,  8, 46, 39, 17,  7, 38, 32, 56,  9, 37, 53, 54, 64, 30, 32, 16, 32,\n",
      "        25,  2, 37, 43, 63, 46, 25, 18,  1, 16, 56, 61, 36, 64, 25, 33, 25, 25,\n",
      "         1, 15, 27,  7, 30, 48,  7, 17, 20, 22, 43,  6, 45,  4, 20,  9, 31, 28,\n",
      "        28, 59, 43, 42, 20, 22,  7, 18, 26, 22, 30, 58, 42, 20, 22, 10, 28, 29,\n",
      "        13, 20, 29,  9, 44, 28, 63, 13, 17, 13,  4, 29, 31, 31, 28, 29, 16, 24,\n",
      "        31, 26, 24, 53,  3, 53,  3, 62, 13, 43,  6, 57, 36, 62,  2,  1, 62, 13,\n",
      "        36,  8,  8, 26, 53, 53,  3, 21, 38, 24, 62, 62, 58, 35, 52,  3, 31, 52,\n",
      "         3, 33, 21, 35, 33, 35, 38, 41, 52, 26,  6,  6, 10, 21, 41, 38, 35, 26,\n",
      "        48, 27, 27, 48, 35,  6,  5, 27, 52,  8, 33, 48,  8, 52, 33,  2, 42,  5,\n",
      "        58])\n",
      "[14, 99, 100, 113, 122, 5, 6, 188, 198, 267, 79, 111, 181, 266, 321, 256, 258, 276, 285, 288, 133, 135, 157, 211, 244, 2, 3, 144, 312, 323, 209, 262, 298, 299, 311, 149, 167, 201, 204, 222, 163, 271, 272, 315, 318, 49, 143, 171, 213, 237, 8, 9, 10, 231, 300, 86, 95, 96, 152, 161, 23, 24, 25, 26, 27, 234, 241, 243, 260, 269, 64, 69, 80, 112, 154, 92, 109, 134, 159, 199, 32, 74, 178, 189, 250, 125, 150, 166, 205, 242, 84, 107, 128, 187, 223, 52, 55, 58, 70, 132, 206, 212, 220, 229, 235, 18, 28, 277, 290, 301, 155, 207, 221, 225, 230, 47, 56, 60, 126, 138, 20, 22, 251, 254, 279, 180, 186, 194, 196, 197, 224, 253, 273, 297, 305, 151, 200, 307, 308, 313, 215, 216, 232, 239, 248, 162, 233, 236, 245, 249, 53, 90, 176, 202, 226, 214, 246, 247, 252, 286, 15, 140, 169, 177, 179, 195, 289, 292, 316, 320, 29, 33, 37, 81, 131, 283, 291, 293, 304, 310, 45, 145, 192, 264, 270, 105, 110, 127, 172, 182, 153, 168, 278, 294, 303, 103, 118, 137, 147, 165, 63, 88, 89, 98, 117, 39, 77, 116, 295, 302, 4, 7, 219, 228, 322, 0, 183, 208, 218, 261, 17, 30, 34, 35, 238, 12, 13, 40, 158, 210, 62, 78, 130, 164, 185, 36, 66, 85, 91, 101, 1, 203, 306, 309, 317, 75, 102, 104, 106, 156, 46, 57, 61, 68, 83, 31, 41, 44, 93, 136, 284, 287, 296, 314, 319, 173, 255, 257, 274, 275, 82, 94, 123, 146, 174, 38, 42, 43, 71, 160, 67, 129, 139, 170, 190, 51, 54, 114, 121, 263, 76, 148, 227, 282, 324, 108, 119, 120, 124, 217, 11, 16, 59, 73, 87, 19, 21, 115, 142, 191, 259, 265, 268, 280, 281, 65, 97, 141, 184, 240, 48, 50, 72, 175, 193]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 325])\n",
      "tensor([[ 15, 100, 101,  ...,  73, 176, 194],\n",
      "        [ 15, 100, 101,  ...,  73, 176, 194],\n",
      "        [ 15, 100, 101,  ...,  73, 176, 194],\n",
      "        ...,\n",
      "        [ 15, 100, 101,  ...,  73, 176, 194],\n",
      "        [ 15, 100, 101,  ...,  73, 176, 194],\n",
      "        [ 15, 100, 101,  ...,  73, 176, 194]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of size (10, 325) with values from 1 to 325\n",
    "tensor = torch.arange(1, 326).repeat(10, 1)  # Repeat the values 1 to 325 across 10 rows\n",
    "\n",
    "# The transform array (make sure the indices are valid for the tensor size)\n",
    "transform = [\n",
    "    14, 99, 100, 113, 122, 5, 6, 188, 198, 267, 79, 111,\n",
    "    181, 266, 321, 256, 258, 276, 285, 288, 133, 135, 157, 211,\n",
    "    244, 2, 3, 144, 312, 323, 209, 262, 298, 299, 311, 149,\n",
    "    167, 201, 204, 222, 163, 271, 272, 315, 318, 49, 143, 171,\n",
    "    213, 237, 8, 9, 10, 231, 300, 86, 95, 96, 152, 161,\n",
    "    23, 24, 25, 26, 27, 234, 241, 243, 260, 269, 64, 69,\n",
    "    80, 112, 154, 92, 109, 134, 159, 199, 32, 74, 178, 189,\n",
    "    250, 125, 150, 166, 205, 242, 84, 107, 128, 187, 223, 52,\n",
    "    55, 58, 70, 132, 206, 212, 220, 229, 235, 18, 28, 277,\n",
    "    290, 301, 155, 207, 221, 225, 230, 47, 56, 60, 126, 138,\n",
    "    20, 22, 251, 254, 279, 180, 186, 194, 196, 197, 224, 253,\n",
    "    273, 297, 305, 151, 200, 307, 308, 313, 215, 216, 232, 239,\n",
    "    248, 162, 233, 236, 245, 249, 53, 90, 176, 202, 226, 214,\n",
    "    246, 247, 252, 286, 15, 140, 169, 177, 179, 195, 289, 292,\n",
    "    316, 320, 29, 33, 37, 81, 131, 283, 291, 293, 304, 310,\n",
    "    45, 145, 192, 264, 270, 105, 110, 127, 172, 182, 153, 168,\n",
    "    278, 294, 303, 103, 118, 137, 147, 165, 63, 88, 89, 98,\n",
    "    117, 39, 77, 116, 295, 302, 4, 7, 219, 228, 322, 0,\n",
    "    183, 208, 218, 261, 17, 30, 34, 35, 238, 12, 13, 40,\n",
    "    158, 210, 62, 78, 130, 164, 185, 36, 66, 85, 91, 101,\n",
    "    1, 203, 306, 309, 317, 75, 102, 104, 106, 156, 46, 57,\n",
    "    61, 68, 83, 31, 41, 44, 93, 136, 284, 287, 296, 314,\n",
    "    319, 173, 255, 257, 274, 275, 82, 94, 123, 146, 174, 38,\n",
    "    42, 43, 71, 160, 67, 129, 139, 170, 190, 51, 54, 114,\n",
    "    121, 263, 76, 148, 227, 282, 324, 108, 119, 120, 124,\n",
    "    217, 11, 16, 59, 73, 87, 19, 21, 115, 142, 191, 259,\n",
    "    265, 268, 280, 281, 65, 97, 141, 184, 240, 48, 50, 72,\n",
    "    175, 193\n",
    "]\n",
    "\n",
    "# Convert transform to a tensor and ensure it's a long tensor for indexing\n",
    "indices = torch.tensor(transform, dtype=torch.long)\n",
    "\n",
    "# Since the original tensor has a size of (10, 325), we need to ensure indices are valid\n",
    "valid_indices = indices[indices < 325]  # Filter to ensure indices are within bounds\n",
    "\n",
    "# Permute the tensor using the valid indices for all 10 rows\n",
    "# Use advanced indexing to create a new tensor\n",
    "permuted_tensor = tensor[:, valid_indices]  # Select columns using the valid indices\n",
    "\n",
    "print(permuted_tensor.shape)  # Should be (10, N), where N is the number of valid indices\n",
    "print(permuted_tensor)  # Display the permuted tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
