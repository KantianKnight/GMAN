{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils_ import log_string, plot_train_val_loss\n",
    "from utils.utils_ import count_parameters, load_data\n",
    "\n",
    "from model.model_ import GMAN\n",
    "from model.train import train\n",
    "from model.test import test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_slot=5, num_his=12, num_pred=12, L=1, K=8, d=8, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, batch_size=32, max_epoch=1, patience=10, learning_rate=0.001, decay_epoch=10, traffic_file='./data/pems-bay.h5', SE_file='./data/SE(PeMS).txt', model_file='./data/GMAN.pkl', log_file='./data/log'\n",
      "loading data...\n",
      "trainX: torch.Size([36458, 12, 325])\t\t trainY: torch.Size([36458, 12, 325])\n",
      "valX:   torch.Size([5189, 12, 325])\t\tvalY:   torch.Size([5189, 12, 325])\n",
      "testX:   torch.Size([10400, 12, 325])\t\ttestY:   torch.Size([10400, 12, 325])\n",
      "mean:   62.7354\t\tstd:   9.4388\n",
      "data loaded!\n",
      "compiling model...\n",
      "trainable parameters: 209,923\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--time_slot', type=int, default=5,\n",
    "                    help='a time step is 5 mins')\n",
    "parser.add_argument('--num_his', type=int, default=12,\n",
    "                    help='history steps')\n",
    "parser.add_argument('--num_pred', type=int, default=12,\n",
    "                    help='prediction steps')\n",
    "parser.add_argument('--L', type=int, default=1,\n",
    "                    help='number of STAtt Blocks')\n",
    "parser.add_argument('--K', type=int, default=8,\n",
    "                    help='number of attention heads')\n",
    "parser.add_argument('--d', type=int, default=8,\n",
    "                    help='dims of each head attention outputs')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7,\n",
    "                    help='training set [default : 0.7]')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.1,\n",
    "                    help='validation set [default : 0.1]')\n",
    "parser.add_argument('--test_ratio', type=float, default=0.2,\n",
    "                    help='testing set [default : 0.2]')\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--max_epoch', type=int, default=1,\n",
    "                    help='epoch to run')\n",
    "parser.add_argument('--patience', type=int, default=10,\n",
    "                    help='patience for early stop')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=10,\n",
    "                    help='decay epoch')\n",
    "parser.add_argument('--traffic_file', default='./data/pems-bay.h5',\n",
    "                    help='traffic file')\n",
    "parser.add_argument('--SE_file', default='./data/SE(PeMS).txt',\n",
    "                    help='spatial embedding file')\n",
    "parser.add_argument('--model_file', default='./data/GMAN.pkl',\n",
    "                    help='save the model to disk')\n",
    "parser.add_argument('--log_file', default='./data/log',\n",
    "                    help='log file')\n",
    "args, unknown = parser.parse_known_args()\n",
    "log = open(args.log_file, 'w')\n",
    "log_string(log, str(args)[10: -1])\n",
    "T = 24 * 60 // args.time_slot  # Number of time steps in one day\n",
    "# load data\n",
    "log_string(log, 'loading data...')\n",
    "(trainX, trainTE, trainY, valX, valTE, valY, testX, testTE,\n",
    "testY, SE, mean, std) = load_data(args)\n",
    "log_string(log, f'trainX: {trainX.shape}\\t\\t trainY: {trainY.shape}')\n",
    "log_string(log, f'valX:   {valX.shape}\\t\\tvalY:   {valY.shape}')\n",
    "log_string(log, f'testX:   {testX.shape}\\t\\ttestY:   {testY.shape}')\n",
    "log_string(log, f'mean:   {mean:.4f}\\t\\tstd:   {std:.4f}')\n",
    "log_string(log, 'data loaded!')\n",
    "del trainX, trainTE, valX, valTE, testX, testTE, mean, std\n",
    "# build model\n",
    "log_string(log, 'compiling model...')\n",
    "\n",
    "model = GMAN(SE, args, bn_decay=0.1)\n",
    "loss_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=args.decay_epoch,\n",
    "                                      gamma=0.9)\n",
    "parameters = count_parameters(model)\n",
    "log_string(log, 'trainable parameters: {:,}'.format(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/GMAN.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "with open('data/GMAN.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS TO CREATE THE MATRIX PERMUTATION \n",
    "\n",
    "\n",
    "# Create a tensor of shape (10, 10)\n",
    "# Fill the second dimension with values from 1 to 10\n",
    "tensor = torch.zeros((10, 10))  # Initialize a tensor of zeros\n",
    "\n",
    "# Assign values from 1 to 10 to the second dimension\n",
    "for i in range(10):\n",
    "    tensor[i,:] = torch.arange(1, 11)  # Fill the first column with values 1 to 10\n",
    "\n",
    "# Print the tensor\n",
    "print(\"Tensor:\")\n",
    "print(tensor)\n",
    "\n",
    "# Given grouping tensor\n",
    "grouping = torch.tensor([2, 1, 2, 1, 2, 1, 2, 1, 2, 1])\n",
    "\n",
    "# Initialize a dictionary to keep track of counts for each number\n",
    "count = {}\n",
    "\n",
    "# Initialize a new tensor to store the transformed values\n",
    "transformed = torch.empty_like(grouping)\n",
    "\n",
    "for index, value in enumerate(grouping):\n",
    "    # If the value is not in the count dictionary, initialize it\n",
    "    if value.item() not in count:\n",
    "        count[value.item()] = 0\n",
    "    \n",
    "    # Increment the count for this value\n",
    "    count[value.item()] += 1\n",
    "    \n",
    "    # Calculate the new value using the formula\n",
    "    k = count[value.item()]  # k is the current count of the number\n",
    "    new_value = 5*(value.item()-1) + k - 1\n",
    "    \n",
    "    # Assign the new value to the transformed tensor\n",
    "    transformed[index] = new_value\n",
    "\n",
    "# Print the original and transformed tensors\n",
    "print(\"Original Tensor:\", grouping)\n",
    "print(\"Transformed Tensor:\", transformed)\n",
    "\n",
    "# Create a matrix of zeros with shape (10, 10)\n",
    "matrix_size = 10\n",
    "matrix = torch.zeros((matrix_size, matrix_size), dtype=torch.int)\n",
    "\n",
    "# Iterate through the tensor and set the corresponding positions to 1\n",
    "for row, value in enumerate(transformed):\n",
    "    matrix[row, value] = 1  # Set the (row, value + 1) position to 1\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(\"Resulting Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "C = torch.matmul(tensor, matrix.float().t())\n",
    "\n",
    "print(C)\n",
    "\n",
    "orig = torch.matmul(C, matrix.float())\n",
    "\n",
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS FOR READING THE INFO IN PEMS-BAY\n",
    "\n",
    "import h5py\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = 'data/pems-bay.h5'  # Replace with your file path\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Function to print dataset information\n",
    "    def print_dataset_info(name, obj):\n",
    "        print(obj)\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"Dataset: {name}\")\n",
    "            print(f\"  Type: {obj.dtype}\")\n",
    "            print(f\"  Shape: {obj.shape}\")\n",
    "            print(f\"  First 5 items: {obj[:5]}\\n\")  # Print first 5 items, if available\n",
    "\n",
    "    # Visit all items in the file\n",
    "    file.visititems(print_dataset_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
