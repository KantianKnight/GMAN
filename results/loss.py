# Training batch: 5 in epoch:0, training batch loss:48.9944
# Training batch: 10 in epoch:0, training batch loss:44.6468
# Training batch: 15 in epoch:0, training batch loss:39.8159
# Training batch: 20 in epoch:0, training batch loss:43.1384
# Training batch: 25 in epoch:0, training batch loss:47.0282
# Training batch: 30 in epoch:0, training batch loss:60.8579
# Training batch: 35 in epoch:0, training batch loss:41.0930
# Training batch: 40 in epoch:0, training batch loss:32.4774
# Training batch: 45 in epoch:0, training batch loss:38.2827
# Training batch: 50 in epoch:0, training batch loss:36.1395
# Training batch: 55 in epoch:0, training batch loss:26.1823
# Training batch: 60 in epoch:0, training batch loss:28.5944
# Training batch: 65 in epoch:0, training batch loss:29.0296
# Training batch: 70 in epoch:0, training batch loss:34.5917
# Training batch: 75 in epoch:0, training batch loss:35.8493
# Training batch: 80 in epoch:0, training batch loss:28.6315
# Training batch: 85 in epoch:0, training batch loss:30.5293
# Training batch: 90 in epoch:0, training batch loss:34.1168
# Training batch: 95 in epoch:0, training batch loss:37.9543
# Training batch: 100 in epoch:0, training batch loss:21.4007
# Training batch: 105 in epoch:0, training batch loss:30.6836
# Training batch: 110 in epoch:0, training batch loss:29.4357
# Training batch: 115 in epoch:0, training batch loss:38.1922
# Training batch: 120 in epoch:0, training batch loss:25.6229
# Training batch: 125 in epoch:0, training batch loss:32.9623
# Training batch: 130 in epoch:0, training batch loss:34.2507
# Training batch: 135 in epoch:0, training batch loss:22.8734
# Training batch: 140 in epoch:0, training batch loss:33.0249
# Training batch: 145 in epoch:0, training batch loss:35.0469
# Training batch: 150 in epoch:0, training batch loss:27.7272
# Training batch: 155 in epoch:0, training batch loss:21.8215
# Training batch: 160 in epoch:0, training batch loss:28.8614
# Training batch: 165 in epoch:0, training batch loss:35.0098
# Training batch: 170 in epoch:0, training batch loss:21.1495
# Training batch: 175 in epoch:0, training batch loss:19.5062
# Training batch: 180 in epoch:0, training batch loss:16.0210
# Training batch: 185 in epoch:0, training batch loss:24.3379
# Training batch: 190 in epoch:0, training batch loss:24.6618
# Training batch: 195 in epoch:0, training batch loss:20.4481
# Training batch: 200 in epoch:0, training batch loss:27.7863
# Training batch: 205 in epoch:0, training batch loss:24.7114
# Training batch: 210 in epoch:0, training batch loss:23.3208
# Training batch: 215 in epoch:0, training batch loss:29.3946
# Training batch: 220 in epoch:0, training batch loss:19.1007
# Training batch: 225 in epoch:0, training batch loss:18.0471
# Training batch: 230 in epoch:0, training batch loss:32.4169
# Training batch: 235 in epoch:0, training batch loss:20.1610
# Training batch: 240 in epoch:0, training batch loss:29.8779
# Training batch: 245 in epoch:0, training batch loss:32.0333
# Training batch: 250 in epoch:0, training batch loss:20.2012
# Training batch: 255 in epoch:0, training batch loss:20.8642
# Training batch: 260 in epoch:0, training batch loss:21.3445
# Training batch: 265 in epoch:0, training batch loss:22.0401
# Training batch: 270 in epoch:0, training batch loss:19.3295
# Training batch: 275 in epoch:0, training batch loss:23.2708
# Training batch: 280 in epoch:0, training batch loss:21.9702
# Training batch: 285 in epoch:0, training batch loss:21.1344
# Training batch: 290 in epoch:0, training batch loss:28.7107
# Training batch: 295 in epoch:0, training batch loss:26.0299
# Training batch: 300 in epoch:0, training batch loss:29.0972
# Training batch: 305 in epoch:0, training batch loss:17.7275
# Training batch: 310 in epoch:0, training batch loss:25.6486
# Training batch: 315 in epoch:0, training batch loss:23.1877
# Training batch: 320 in epoch:0, training batch loss:21.0148
# Training batch: 325 in epoch:0, training batch loss:19.5422
# Training batch: 330 in epoch:0, training batch loss:18.5384
# Training batch: 335 in epoch:0, training batch loss:28.7678
# Training batch: 340 in epoch:0, training batch loss:30.2733
# Training batch: 345 in epoch:0, training batch loss:18.6144
# Training batch: 350 in epoch:0, training batch loss:19.4242
# Training batch: 355 in epoch:0, training batch loss:22.9451
# Training batch: 360 in epoch:0, training batch loss:25.5513
# Training batch: 365 in epoch:0, training batch loss:23.5718
# Training batch: 370 in epoch:0, training batch loss:22.0577
# Training batch: 375 in epoch:0, training batch loss:23.5071
# Training batch: 380 in epoch:0, training batch loss:17.9110
# Training batch: 385 in epoch:0, training batch loss:21.7665
# Training batch: 390 in epoch:0, training batch loss:23.2256
# Training batch: 395 in epoch:0, training batch loss:28.2381
# Training batch: 400 in epoch:0, training batch loss:23.3779
# Training batch: 405 in epoch:0, training batch loss:14.8724
# Training batch: 410 in epoch:0, training batch loss:27.5587
# Training batch: 415 in epoch:0, training batch loss:21.8882
# Training batch: 420 in epoch:0, training batch loss:23.1002
# Training batch: 425 in epoch:0, training batch loss:15.7407
# Training batch: 430 in epoch:0, training batch loss:20.3202
# Training batch: 435 in epoch:0, training batch loss:17.5692
# Training batch: 440 in epoch:0, training batch loss:19.2343
# Training batch: 445 in epoch:0, training batch loss:19.9721
# Training batch: 450 in epoch:0, training batch loss:20.9998
# Training batch: 455 in epoch:0, training batch loss:23.3610
# Training batch: 460 in epoch:0, training batch loss:22.4742
# Training batch: 465 in epoch:0, training batch loss:21.2431
# Training batch: 470 in epoch:0, training batch loss:18.6950
# Training batch: 475 in epoch:0, training batch loss:19.9807
# Training batch: 480 in epoch:0, training batch loss:22.3057
# Training batch: 485 in epoch:0, training batch loss:44.8658
# Training batch: 490 in epoch:0, training batch loss:18.3950
# Training batch: 495 in epoch:0, training batch loss:22.0702
# Training batch: 500 in epoch:0, training batch loss:30.6002
# Training batch: 505 in epoch:0, training batch loss:23.3703
# Training batch: 510 in epoch:0, training batch loss:19.9950
# Training batch: 515 in epoch:0, training batch loss:21.9028
# Training batch: 520 in epoch:0, training batch loss:17.1024
# Training batch: 525 in epoch:0, training batch loss:18.9616
# Training batch: 530 in epoch:0, training batch loss:18.3512
# Training batch: 535 in epoch:0, training batch loss:20.4696
# Training batch: 540 in epoch:0, training batch loss:18.3507
# Training batch: 545 in epoch:0, training batch loss:17.2892
# Training batch: 550 in epoch:0, training batch loss:20.0923
# Training batch: 555 in epoch:0, training batch loss:18.6466
# Training batch: 560 in epoch:0, training batch loss:26.4325
# Training batch: 565 in epoch:0, training batch loss:26.2246
# Training batch: 570 in epoch:0, training batch loss:13.3435
# Training batch: 575 in epoch:0, training batch loss:21.2177
# Training batch: 580 in epoch:0, training batch loss:19.7056
# Training batch: 585 in epoch:0, training batch loss:25.5492
# Training batch: 590 in epoch:0, training batch loss:22.2121
# Training batch: 595 in epoch:0, training batch loss:25.6997
# Training batch: 600 in epoch:0, training batch loss:30.4588
# Training batch: 605 in epoch:0, training batch loss:21.4623
# Training batch: 610 in epoch:0, training batch loss:19.4597
# Training batch: 615 in epoch:0, training batch loss:19.2985
# Training batch: 620 in epoch:0, training batch loss:19.0393
# Training batch: 625 in epoch:0, training batch loss:16.4920
# Training batch: 630 in epoch:0, training batch loss:27.4737
# Training batch: 635 in epoch:0, training batch loss:26.1461
# Training batch: 640 in epoch:0, training batch loss:23.7813
# Training batch: 645 in epoch:0, training batch loss:18.6368
# Training batch: 650 in epoch:0, training batch loss:18.4271
# Training batch: 655 in epoch:0, training batch loss:20.0275
# Training batch: 660 in epoch:0, training batch loss:20.3511
# Training batch: 665 in epoch:0, training batch loss:19.6931
# Training batch: 670 in epoch:0, training batch loss:45.1082
# Training batch: 675 in epoch:0, training batch loss:18.6455
# Training batch: 680 in epoch:0, training batch loss:20.2452
# Training batch: 685 in epoch:0, training batch loss:29.7249
# Training batch: 690 in epoch:0, training batch loss:16.4212
# Training batch: 695 in epoch:0, training batch loss:22.9588
# Training batch: 700 in epoch:0, training batch loss:14.7623
# Training batch: 705 in epoch:0, training batch loss:41.5820
# Training batch: 710 in epoch:0, training batch loss:20.8929
# Training batch: 715 in epoch:0, training batch loss:20.3281
# Training batch: 720 in epoch:0, training batch loss:23.5307
# Training batch: 725 in epoch:0, training batch loss:15.7611
# Training batch: 730 in epoch:0, training batch loss:23.8294
# Training batch: 735 in epoch:0, training batch loss:15.3948
# Training batch: 740 in epoch:0, training batch loss:21.8231
# Training batch: 745 in epoch:0, training batch loss:19.2563
# Training batch: 750 in epoch:0, training batch loss:21.0419
# Training batch: 755 in epoch:0, training batch loss:24.3563
# Training batch: 760 in epoch:0, training batch loss:19.0531
# Training batch: 765 in epoch:0, training batch loss:28.9622
# Training batch: 770 in epoch:0, training batch loss:18.9068
# Training batch: 775 in epoch:0, training batch loss:20.4856
# Training batch: 780 in epoch:0, training batch loss:29.9334
# Training batch: 785 in epoch:0, training batch loss:20.2871
# Training batch: 790 in epoch:0, training batch loss:16.2657
# Training batch: 795 in epoch:0, training batch loss:26.8453
# Training batch: 800 in epoch:0, training batch loss:17.4462
# Training batch: 805 in epoch:0, training batch loss:17.7090
# Training batch: 810 in epoch:0, training batch loss:32.0019
# Training batch: 815 in epoch:0, training batch loss:21.9874
# Training batch: 820 in epoch:0, training batch loss:17.4712
# Training batch: 825 in epoch:0, training batch loss:26.9158
# Training batch: 830 in epoch:0, training batch loss:32.8230
# Training batch: 835 in epoch:0, training batch loss:17.8580
# Training batch: 840 in epoch:0, training batch loss:19.2417
# Training batch: 845 in epoch:0, training batch loss:16.0383
# Training batch: 850 in epoch:0, training batch loss:20.9118
# Training batch: 855 in epoch:0, training batch loss:17.8820
# Training batch: 860 in epoch:0, training batch loss:22.5960
# Training batch: 865 in epoch:0, training batch loss:22.8981
# Training batch: 870 in epoch:0, training batch loss:17.4569
# Training batch: 875 in epoch:0, training batch loss:32.1896
# Training batch: 880 in epoch:0, training batch loss:19.0955
# Training batch: 885 in epoch:0, training batch loss:23.7554
# Training batch: 890 in epoch:0, training batch loss:20.5915
# Training batch: 895 in epoch:0, training batch loss:15.5897
# Training batch: 900 in epoch:0, training batch loss:20.5023
# Training batch: 905 in epoch:0, training batch loss:22.3525
# Training batch: 910 in epoch:0, training batch loss:21.3571
# Training batch: 915 in epoch:0, training batch loss:20.9370
# Training batch: 920 in epoch:0, training batch loss:16.9015
# Training batch: 925 in epoch:0, training batch loss:18.1072
# Training batch: 930 in epoch:0, training batch loss:17.9285
# Training batch: 935 in epoch:0, training batch loss:18.0770
# Training batch: 940 in epoch:0, training batch loss:18.9017
# Training batch: 945 in epoch:0, training batch loss:17.6572
# Training batch: 950 in epoch:0, training batch loss:14.2661
# Training batch: 955 in epoch:0, training batch loss:20.9476
# Training batch: 960 in epoch:0, training batch loss:22.6699
# Training batch: 965 in epoch:0, training batch loss:19.9115
# Training batch: 970 in epoch:0, training batch loss:16.4787
# Training batch: 975 in epoch:0, training batch loss:16.6396
# Training batch: 980 in epoch:0, training batch loss:22.1752
# Training batch: 985 in epoch:0, training batch loss:16.0053
# Training batch: 990 in epoch:0, training batch loss:15.4300
# Training batch: 995 in epoch:0, training batch loss:16.1132
# Training batch: 1000 in epoch:0, training batch loss:21.0468
# Training batch: 1005 in epoch:0, training batch loss:19.6442
# Training batch: 1010 in epoch:0, training batch loss:24.9377
# Training batch: 1015 in epoch:0, training batch loss:16.8775
# Training batch: 1020 in epoch:0, training batch loss:18.5323
# Training batch: 1025 in epoch:0, training batch loss:16.7560
# Training batch: 1030 in epoch:0, training batch loss:15.7789
# Training batch: 1035 in epoch:0, training batch loss:19.4075
# Training batch: 1040 in epoch:0, training batch loss:14.6237
# Training batch: 1045 in epoch:0, training batch loss:29.6889
# Training batch: 1050 in epoch:0, training batch loss:18.2106
# Training batch: 1055 in epoch:0, training batch loss:17.6043
# Training batch: 1060 in epoch:0, training batch loss:24.6181
# Training batch: 1065 in epoch:0, training batch loss:16.8832
# Training batch: 1070 in epoch:0, training batch loss:29.4313
# Training batch: 1075 in epoch:0, training batch loss:19.5124
# Training batch: 1080 in epoch:0, training batch loss:18.8857
# Training batch: 1085 in epoch:0, training batch loss:19.3722
# Training batch: 1090 in epoch:0, training batch loss:26.8807
# Training batch: 1095 in epoch:0, training batch loss:18.9603
# Training batch: 1100 in epoch:0, training batch loss:17.0376
# Training batch: 1105 in epoch:0, training batch loss:24.5000
# Training batch: 1110 in epoch:0, training batch loss:28.1448
# Training batch: 1115 in epoch:0, training batch loss:13.8116
# Training batch: 1120 in epoch:0, training batch loss:21.5987
# Training batch: 1125 in epoch:0, training batch loss:19.1873
# Training batch: 1130 in epoch:0, training batch loss:13.1678
# Training batch: 1135 in epoch:0, training batch loss:18.6532
# Training batch: 1140 in epoch:0, training batch loss:15.6429

# Get the average of the training batch loss values above

import re

# Extract all loss values from the comments
loss_values = re.findall(r'training batch loss:(\d+\.\d+)', open(__file__).read())
loss_values = [float(loss) for loss in loss_values]

# Calculate the average loss
average_loss = sum(loss_values) / len(loss_values)

print(f'Average training batch loss: {average_loss:.4f}')